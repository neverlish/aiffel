{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) 필요한 모듈 import하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) 데이터 준비\n",
    "\n",
    "load_breast_cancer 메서드를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) 데이터 이해하기\n",
    "\n",
    "지피지기면 백전불태! 다루어야 할 데이터를 자세히 살펴봅시다.\n",
    "\n",
    "- Feature Data 지정하기\n",
    "- Label Data 지정하기\n",
    "- Target Names 출력해 보기\n",
    "- 데이터 Describe 해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = data.data\n",
    "target_data = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['malignant', 'benign'], dtype='<U9')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "       'smoothness error', 'compactness error', 'concavity error',\n",
       "       'concave points error', 'symmetry error',\n",
       "       'fractal dimension error', 'worst radius', 'worst texture',\n",
       "       'worst perimeter', 'worst area', 'worst smoothness',\n",
       "       'worst compactness', 'worst concavity', 'worst concave points',\n",
       "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (4) train, test 데이터 분리\n",
    "\n",
    "모델 학습과 테스트용 문제지와 정답지를 준비해 봅시다.\n",
    "X_train, X_test, y_train, y_test를 생성하는 방법을 참고해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature_data, \n",
    "    target_data, \n",
    "    test_size=0.4, \n",
    "    random_state=7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (5) 다양한 모델로 학습시켜보기\n",
    "\n",
    "학습데이터 X_train, y_train 을 활용해 분류기 모델을 만들어 봅시다. 어떤 모델이 가장 좋은 성능을 보일까요?\n",
    "\n",
    "- Decision Tree 사용해 보기\n",
    "- Random Forest 사용해 보기\n",
    "- SVM 사용해 보기\n",
    "- SGD Classifier 사용해 보기\n",
    "- Logistic Regression 사용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86        73\n",
      "           1       0.93      0.94      0.94       155\n",
      "\n",
      "    accuracy                           0.91       228\n",
      "   macro avg       0.90      0.90      0.90       228\n",
      "weighted avg       0.91      0.91      0.91       228\n",
      "\n",
      "---------------\n",
      "random_forest REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95        73\n",
      "           1       0.96      0.99      0.98       155\n",
      "\n",
      "    accuracy                           0.97       228\n",
      "   macro avg       0.97      0.96      0.96       228\n",
      "weighted avg       0.97      0.97      0.97       228\n",
      "\n",
      "---------------\n",
      "svm REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.78      0.87        73\n",
      "           1       0.91      0.99      0.95       155\n",
      "\n",
      "    accuracy                           0.93       228\n",
      "   macro avg       0.94      0.89      0.91       228\n",
      "weighted avg       0.93      0.93      0.92       228\n",
      "\n",
      "---------------\n",
      "sgd REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.90      0.80        73\n",
      "           1       0.95      0.83      0.88       155\n",
      "\n",
      "    accuracy                           0.85       228\n",
      "   macro avg       0.83      0.86      0.84       228\n",
      "weighted avg       0.87      0.85      0.85       228\n",
      "\n",
      "---------------\n",
      "logistic REPORT\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.86      0.91        73\n",
      "           1       0.94      0.98      0.96       155\n",
      "\n",
      "    accuracy                           0.94       228\n",
      "   macro avg       0.95      0.92      0.93       228\n",
      "weighted avg       0.94      0.94      0.94       228\n",
      "\n",
      "---------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyeonjinho/opt/anaconda3/envs/aiffel/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "\n",
    "decision_tree = DecisionTreeClassifier()\n",
    "random_forest = RandomForestClassifier()\n",
    "svm_model = SVC()\n",
    "sgd_model = SGDClassifier()\n",
    "logistic_model = LogisticRegression()\n",
    "models = [\n",
    "    ('decision', decision_tree),\n",
    "    ('random_forest', random_forest),\n",
    "    ('svm', svm_model),\n",
    "    ('sgd', sgd_model),\n",
    "    ('logistic', logistic_model)\n",
    "]\n",
    "\n",
    "for model_name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(model_name + ' REPORT')\n",
    "    print(report)\n",
    "    print('---------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (6) 모델을 평가해 보기\n",
    "\n",
    "학습된 모델들의 테스트데이터 예측 결과를 어떻게 해석해야 할까요? 모델의 성능을 평가하는 지표로는 무엇이 좋을까요? sklearn.metrics 에서 제공하는 평가지표 중 적절한 것을 선택해 보세요. 선택하신 이유도 설명해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A: random forest가 적절한 모델이라 생각합니다. precision과 recall 모두 가장 높기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
